---
title: "Lags in log-bin models"
author: "Will Simmons"
date: "2/7/2020"
output: html_document
editor_options: 
  chunk_output_type: console
---

NOTE: NEED TO REDO LAG ANALYSES WITH FULL ENACTS DATASET (i.e. have data on all lags since only NSP data are incomplete - not ENACTS!)

# Overview

Purpose of these analyses:

  * Investigate what lag periods, if any, reveal significant associations between anomalously high temperatures and rate of wasting
  * For these lags, conduct future analyses taking lags into account
    + Conditional (Quasi-) Poisson a la Armstrong et al 2014 (BMC Med Res Methodol)?
    + Distributed Lag Nonlinear Models?
    
# Investigating lags

## Setup + data import

First, I'll set up our libraries and data. Climate and nutrition data come from the Bangladesh Nutrition Surveillance Project (NSP) and the Enhancing National Climate Services (ENACTS) initiative.

In the `nsp_enacts` dataset, climate and nutrition data have been matched by (1) geographic survey area (22 areas total) and (2) calendar date (ranging from 1990-Jun-02 to 2006-Nov-03).

```{r data}

library(tidyverse)
library(readxl)
library(knitr)
library(kableExtra)
library(logbin)
library(tsModel)

## reading in data
enacts = readRDS(file = './data/enacts')
nsp_enacts = readRDS(file = './data/nsp_enacts')

## disabling scientific notation
options(scipen = 999)
```

## Creating temperature anomaly variables

Next, I'll create a variable in the dataset that indicates the extent of anomalous temperature from the survey area's long-term mean for a given (1) day, (2) week, and (3) month (3 separate variables). 

For now, I'll only focus on *positive* anomalies for *temperature*. Later, I might also consider positive vs. negative anomalies, and anomalies in temperature vs. precipitation.

```{r anomaly}

# workflow
  # number days 1-365, weeks 1-52, months 1-12
    # exclude leap days for simplicity
    # week 53 = week 52 (week 52 of each year will include extra day)
  # anomalies
    # group by area
    # day:
      # group by day of year
      # summarize to find average
      # ungroup
    # week and month similar to above, but create week-within-year and month-within-year means to compare to across-year means
    # ungroup by area
    # all observations should have day, week, and month averages specific to area

## duplicating dataset
data = enacts

## formatting day/week/month data
anomalies =
  data %>% 
  mutate(
    surv_area = as.character(surv_area),    ## keeps from coverting first category (area 4) into numeric 1 (since factor = 1)
    surv_area = as.numeric(surv_area)
  ) %>% 
  ungroup() %>%                    ## data were grouped by `date`
  filter(                          ## excluding leap days for simplicity (can figure out later) - 88 area-days
    !(lubridate::day(date) == 29 &
    lubridate::month(date) == 02)
  ) %>% 
  mutate(
    doy = lubridate::yday(date),    ## day of year
    doy = as.numeric(doy),
    woy = lubridate::week(date),    ## week of year   
    woy = as.numeric(woy),
    moy = lubridate::month(date),   ## month of year
    moy = as.numeric(moy),
    woy = case_when(woy == 53 ~ 52, ## folding 1 day of 53rd week as 8th day of 52nd week for simplicity
                    TRUE ~ woy),
    year = lubridate::year(date)
  ) %>% 
  
## creating anomaly variables

  ## day avg
  group_by(surv_area, doy) %>% 
  mutate(
    
    day_avg_tmax = mean(tmax),                         ## day-of-year avg for each area
    day_pos_t_anomaly = tmax - day_avg_tmax,           ## pos t anomaly for each area day
    day_pos_t_anomaly = case_when(                     ## excluding anomalous values less than 0
      day_pos_t_anomaly < 0 ~ 0,
      TRUE ~ day_pos_t_anomaly
    ),
    
    day_avg_p = mean(precip),                         ## day-of-year avg for each area
    day_p_anomaly = precip - day_avg_p,               ## pos p anomaly for each area day
    day_pos_p_anomaly = case_when(                     ## excluding anomalous values less than 0
      day_p_anomaly < 0 ~ 0,
      TRUE ~ day_p_anomaly
    ),
    
    day_neg_p_anomaly = case_when( 
      day_pos_p_anomaly > 0 ~ 0,                     ## neg p anomaly for each area day
      TRUE ~ day_p_anomaly                          ## excluding anomalous values greater than 0
    )
    
  ) %>% 
  select(-day_p_anomaly) %>%                         ## getting rid of overall p anomaly created as intermediate step above
  ungroup() %>%
  
  ## week-within-year avg
  group_by(surv_area, year, woy) %>%                  ## grouping by area, year, and week
  mutate(
    wkyr_avg_tmax = mean(tmax),                        ## avg tmax for week-within-year
    wkyr_avg_p = mean(precip)
  ) %>% 
  ungroup() %>% 
  
  ## week-across-years avg and anomaly
  group_by(surv_area, woy) %>%                        ## grouping by area and week (1-52)
  mutate(
    wk_avg_tmax = mean(tmax),                         ## avg tmax for week-across-years
    wk_pos_t_anomaly = 
      wkyr_avg_tmax - wk_avg_tmax,                    ## anomaly for each area (how far is week-within-year from week-across-years)
    wk_pos_t_anomaly = case_when(                     ## excluding anomalous values less than 0
      wk_pos_t_anomaly < 0 ~ 0,
      TRUE ~ wk_pos_t_anomaly
    ),
    
    wk_avg_p = mean(precip),                         ## avg p for week-across-years
    wk_p_anomaly = 
      wkyr_avg_p - wk_avg_p,                          ## anomaly for each area (how far is week-within-year from week-across-years)
    wk_pos_p_anomaly = case_when(                     ## excluding anomalous values less than 0
      wk_p_anomaly < 0 ~ 0,
      TRUE ~ wk_p_anomaly
    ),
    
    wk_neg_p_anomaly = case_when(                     ## excluding anomalous values grtr than 0
      wk_p_anomaly > 0 ~ 0,
      TRUE ~ wk_p_anomaly
    )
  ) %>% 
  select(-wk_p_anomaly) %>%                          ## gtting rid of overall anomaly variable for wk
  ungroup() %>% 
  
  ## month-within-year avg
  group_by(surv_area, year, moy) %>%                  ## grouping by area, year, and month
  mutate(
    moyr_avg_tmax = mean(tmax),
    moyr_avg_p = mean(precip)
  ) %>% 
  ungroup() %>% 
  
  ## month-across-years avg and anomaly
  group_by(surv_area, moy) %>%                        ## grouping by area and month
  mutate(
    mo_avg_tmax = mean(tmax),
    mo_pos_t_anomaly = 
      moyr_avg_tmax - mo_avg_tmax,                    ## anomaly for each area (how far is mo-within-year from mo-across-years)
    mo_pos_t_anomaly = case_when(                     ## excluding anomalous values less than 0
      mo_pos_t_anomaly < 0 ~ 0,
      TRUE ~ mo_pos_t_anomaly
    ),
    
    mo_avg_p = mean(precip),
    mo_p_anomaly = 
      moyr_avg_p - mo_avg_p,                    ## anomaly for each area (how far is mo-within-year from mo-across-years)
    mo_pos_p_anomaly = case_when(                     ## excluding anomalous values less than 0
      mo_p_anomaly < 0 ~ 0,
      TRUE ~ mo_p_anomaly
    ),
    
    mo_avg_p = mean(precip),
    mo_p_anomaly = 
      moyr_avg_p - mo_avg_p,                    ## anomaly for each area (how far is mo-within-year from mo-across-years)
    mo_neg_p_anomaly = case_when(                     ## excluding anomalous values grtr than 0
      mo_p_anomaly > 0 ~ 0,
      TRUE ~ mo_p_anomaly
    )
  
  ) %>% 
  select(-mo_p_anomaly) %>%                    ## gtting rid of overall anomaly variable for month
  ungroup()

```

I excluded 88 climate area-day observations that occurred on leap days for simplicity's sake, resulting in a new dataframe `anomalies` with a rowcount of `r nrow(anomalies)`.

A bit confusing, but here's what we have, all specific to a respective geographic area:

  * `tmax`: day-within-year maximum temperature
  * `day_avg_tmax`: longterm day-across-years mean `tmax`
  * `day_pos_t_anomaly`: how far is `tmax` on a given day from the longterm mean `tmax` across years on that day of year (1-365)?
  * `day_pos_p_anomaly`: similar to above, but for precipitation
  * `day_neg_p_anomaly`: I coded a negative precipitation anomaly, since low rainfall could be important for crops, etc.
  
  * `wkyr_avg_tmax`: week-within-year mean `tmax`
  * `wk_avg_tmax`: longterm week-across-years mean `tmax`
  * `wk_pos_t_anomaly`: how far is mean `tmax` across a given week from the longterm mean across years for that week of the year (1-52)?
  * `wk_pos_p_anomaly`: similar to above, but for precipitation
  * `wk_neg_p_anomaly`: same but negative precipitation anomalies on a weekly scale
  
  * `moyr_avg_tmax`: month-within-year mean `tmax`
  * `mo_avg_tmax`: longterm month-across-years mean `tmax` 
  * `mo_pos_t_anomaly`: how far is mean `tmax` across a given month from the longerm mean across years for that month of the year (1-12)?
  * `mo_pos_p_anomaly`: similar to above, but for precipitation
  * `mo_neg_p_anomaly`: same but negative precipitation anomalies on a monthly scale
  
  * NOTE: all temp anomaly variables are positive anomalies only (i.e. anomalies below 0 have been recoded as 0). For precip, both positive and negative anomalies have been coded separately.

## Creating lags within ENACTS dataset

Since we have a consistent timeseries within our ENACTS data, we'll create our lagged variables here to save us the trouble of working in a table that also includes inconsistently timed NSP data. We'll join to NSP data in the next step.

```{r lag_data}

## week-level only first - using woy (PROBLEM - non-cyclic date structure, so last week of Dec isn't lagged as first of next Jan) ####

# anomalies_lag =
#   anomalies %>% 
#   select(                  ## extract week-level data
#     surv_area,
#     woy,
#     year,
#     wkyr_avg_tmax,
#     wk_avg_tmax,
#     wk_pos_t_anomaly
#   ) %>% 
#   distinct() %>%              ## make only week-level by calling distinct() --> merges like rows
#   group_by(
#     surv_area,
#     year,
#     woy) %>% 
#   arrange(surv_area) %>% 
#   group_by(
#     surv_area,
#     year
#   ) %>% 
#   mutate(                                                           ## creating lags within weekly ENACTS dataset
#     lag1 = lag(wk_pos_t_anomaly, 1, order_by = woy),
#     lag2 = lag(wk_pos_t_anomaly, 2, order_by = woy),
#     lag3 = lag(wk_pos_t_anomaly, 3, order_by = woy),
#     lag4 = lag(wk_pos_t_anomaly, 4, order_by = woy),
#     lag5 = lag(wk_pos_t_anomaly, 5, order_by = woy),
#     lag6 = lag(wk_pos_t_anomaly, 6, order_by = woy),
#     lag7 = lag(wk_pos_t_anomaly, 7, order_by = woy),
#     lag8 = lag(wk_pos_t_anomaly, 8, order_by = woy)
#   )



# trying yearweek format to make dates cyclic - e.g. lag 1 for Wk 1 Jan 1999 will be value from last wk of Dec 1998 ###############

## ** wonky with how I've set up no leap days / 52 wks **

# anomalies_lag2 =
#   anomalies %>% 
#   mutate(
#     woy2 = tsibble::yearweek(date)
#   ) %>% 
#   select(                  ## extract week-level data
#     surv_area,
#     woy2,
#     # woy,
#     # year,
#     wkyr_avg_tmax,
#     wk_avg_tmax,
#     wk_pos_t_anomaly
#   ) %>% 
#   distinct() %>%              ## make only week-level by calling distinct() --> merges like rows
#   arrange(
#     surv_area
#   ) %>% 
#   group_by(
#     surv_area
#     # year,
#     # woy
#   ) %>% 
#   mutate(                                                           ## creating lags within weekly ENACTS dataset
#     lag1 = lag(wk_pos_t_anomaly, 1, order_by = woy2),
#     lag2 = lag(wk_pos_t_anomaly, 2, order_by = woy2),
#     lag3 = lag(wk_pos_t_anomaly, 3, order_by = woy2),
#     lag4 = lag(wk_pos_t_anomaly, 4, order_by = woy2),
#     lag5 = lag(wk_pos_t_anomaly, 5, order_by = woy2),
#     lag6 = lag(wk_pos_t_anomaly, 6, order_by = woy2),
#     lag7 = lag(wk_pos_t_anomaly, 7, order_by = woy2),
#     lag8 = lag(wk_pos_t_anomaly, 8, order_by = woy2)
#   )




# trying weektotal variable constructed from woy + 52*(1990 - year) ############

anomalies_lag3 =
  anomalies %>%
  mutate(
    woy3 = woy + 52*(year - 1990)
  ) %>%
  select(                  ## extract week-level data
    surv_area,
    woy3,
    woy,
    year,
    wkyr_avg_tmax,
    wk_avg_tmax,
    wk_pos_t_anomaly,
    wkyr_avg_p,
    wk_avg_p,
    wk_pos_p_anomaly,
    wk_neg_p_anomaly
  ) %>%
  distinct() %>%              ## make only week-level by calling distinct() --> merges like rows
  arrange(
    surv_area
  ) %>%
  group_by(
    surv_area
    # year,
    # woy
  ) %>%
  mutate(                                                           ## creating lags within weekly ENACTS dataset
    lag1_t = lag(wk_pos_t_anomaly, 1, order_by = woy3),             ## lags in pos t anomalies by week
    lag2_t = lag(wk_pos_t_anomaly, 2, order_by = woy3),
    lag3_t = lag(wk_pos_t_anomaly, 3, order_by = woy3),
    lag4_t = lag(wk_pos_t_anomaly, 4, order_by = woy3),
    lag5_t = lag(wk_pos_t_anomaly, 5, order_by = woy3),
    lag6_t = lag(wk_pos_t_anomaly, 6, order_by = woy3),
    lag7_t = lag(wk_pos_t_anomaly, 7, order_by = woy3),
    lag8_t = lag(wk_pos_t_anomaly, 8, order_by = woy3),
    lag9_t = lag(wk_pos_t_anomaly, 9,  order_by = woy3),
    lag10_t = lag(wk_pos_t_anomaly, 10, order_by = woy3),
    lag11_t = lag(wk_pos_t_anomaly, 11, order_by = woy3),
    lag12_t = lag(wk_pos_t_anomaly, 12, order_by = woy3),
    lag13_t = lag(wk_pos_t_anomaly, 13, order_by = woy3),
    lag14_t = lag(wk_pos_t_anomaly, 14, order_by = woy3),
    lag15_t = lag(wk_pos_t_anomaly, 15, order_by = woy3),
    lag16_t = lag(wk_pos_t_anomaly, 16, order_by = woy3),
    
    lag1_p_pos = lag(wk_pos_p_anomaly, 1, order_by = woy3),            ## lags in pos p anomalies by wk
    lag2_p_pos = lag(wk_pos_p_anomaly, 2, order_by = woy3),
    lag3_p_pos = lag(wk_pos_p_anomaly, 3, order_by = woy3),
    lag4_p_pos = lag(wk_pos_p_anomaly, 4, order_by = woy3),
    lag5_p_pos = lag(wk_pos_p_anomaly, 5, order_by = woy3),
    lag6_p_pos = lag(wk_pos_p_anomaly, 6, order_by = woy3),
    lag7_p_pos = lag(wk_pos_p_anomaly, 7, order_by = woy3),
    lag8_p_pos = lag(wk_pos_p_anomaly, 8, order_by = woy3),
    lag9_p_pos = lag(wk_pos_p_anomaly, 9,  order_by = woy3),
    lag10_p_pos = lag(wk_pos_p_anomaly, 10, order_by = woy3),
    lag11_p_pos = lag(wk_pos_p_anomaly, 11, order_by = woy3),
    lag12_p_pos = lag(wk_pos_p_anomaly, 12, order_by = woy3),
    lag13_p_pos = lag(wk_pos_p_anomaly, 13, order_by = woy3),
    lag14_p_pos = lag(wk_pos_p_anomaly, 14, order_by = woy3),
    lag15_p_pos = lag(wk_pos_p_anomaly, 15, order_by = woy3),
    lag16_p_pos = lag(wk_pos_p_anomaly, 16, order_by = woy3),
    
    lag1_p_neg = lag(wk_neg_p_anomaly, 1, order_by = woy3),            ## lags in neg p anomalies by wk
    lag2_p_neg = lag(wk_neg_p_anomaly, 2, order_by = woy3),
    lag3_p_neg = lag(wk_neg_p_anomaly, 3, order_by = woy3),
    lag4_p_neg = lag(wk_neg_p_anomaly, 4, order_by = woy3),
    lag5_p_neg = lag(wk_neg_p_anomaly, 5, order_by = woy3),
    lag6_p_neg = lag(wk_neg_p_anomaly, 6, order_by = woy3),
    lag7_p_neg = lag(wk_neg_p_anomaly, 7, order_by = woy3),
    lag8_p_neg = lag(wk_neg_p_anomaly, 8, order_by = woy3),
    lag9_p_neg = lag(wk_neg_p_anomaly, 9,  order_by = woy3),
    lag10_p_neg = lag(wk_neg_p_anomaly, 10, order_by = woy3),
    lag11_p_neg = lag(wk_neg_p_anomaly, 11, order_by = woy3),
    lag12_p_neg = lag(wk_neg_p_anomaly, 12, order_by = woy3),
    lag13_p_neg = lag(wk_neg_p_anomaly, 13, order_by = woy3),
    lag14_p_neg = lag(wk_neg_p_anomaly, 14, order_by = woy3),
    lag15_p_neg = lag(wk_neg_p_anomaly, 15, order_by = woy3),
    lag16_p_neg = lag(wk_neg_p_anomaly, 16, order_by = woy3)

  )

## I think this worked! anomalies_lag3

```


## Join with NSP data

Since I was just working with ENACTS data above (full climate data), I'll now join with NSP data (nutrition data).

```{r NSP_join}

nsp_anomalies_wk_lag =
  nsp_enacts %>% 
  filter(                          ## excluding leap days for simplicity (can figure out later) - 88 area-days
  !(lubridate::day(dov) == 29 &
  lubridate::month(dov) == 02)
) %>% 
  mutate(
    doy = lubridate::yday(dov),    ## day of year
    doy = as.numeric(doy),
    woy = lubridate::week(dov),    ## week of year   
    woy = as.numeric(woy),
    moy = lubridate::month(dov),   ## month of year
    moy = as.numeric(moy),
    woy = case_when(woy == 53 ~ 52, ## folding 1 day of 53rd week as 8th day of 52nd week for simplicity
                    TRUE ~ woy),
    year = lubridate::year(dov),
    woy3 = woy + 52*(year - 1990)
  ) %>% 
  mutate(
    surv_area = as.numeric(surv_area),
    ) %>%      ## to match formats to join
  select(-precip, -tmax, -tmin, -area_name, date = dov,
         -woy, -year) %>% 
  full_join(
    .,
    anomalies_lag3,
    by = c('surv_area', 'woy3')
  ) %>% 
  
  arrange(
    surv_area,
    date
  ) %>% 
  
  ## creating wasting variable
  mutate(
  wasting = case_when(
    zwfl < -2 ~ '1',
    zwfl >= -2 ~ '0',
    TRUE ~ ""
  ),
  wasting = as.logical(as.numeric(wasting))
) %>% 
  
  select(date, surv_area, ageindays, zwfl, wasting, everything())      ## reordering + removing area_name (add back)

## check to make sure lags worked!

```

## Crude log-binomial models with lags

Now, I'll construct log-binomial models to calculate prevalence ratios (PRs) at differing lags and plot the results.

Lagged crude models for weekly anomalies (lags at 0 to 8 weeks):

```{r crude_models}

# ## crude log-binomial model: daily positive tmax anomaly ######
# logbin_crude_t_pos_day =
#   logbin(
#     data = nsp_anomalies_wk_lag,
#     formula = wasting ~ day_pos_t_anomaly,
#     method = 'em',
#     accelerate = 'squarem' # these two options speed the convergence
#   )
# 
# logbin_crude_t_pos_day %>% 
#   broom::tidy()



## adjusted model: weekly positive tmax anomaly #########
# logbin_crude_t_pos_wk =
#   nsp_anomalies_wk_lag %>% 
#   mutate(
#     surv_area = as.factor(surv_area),
#     woy = as.factor(woy),
#     year = as.factor(year),
#     moy = as.factor(moy)
#   ) %>% 
#   logbin(
#     data = .,
#     formula = wasting ~ wk_pos_t_anomaly +
#       surv_area + moy + year,
#     method = 'em',
#     accelerate = 'squarem' # these two options speed the convergence
#   ) %>% 
#     broom::tidy()

# ## crude model: monthly positive tmax anomaly ###########
# logbin_crude_t_pos_mo =
#   logbin(
#     data = nsp_anomalies,
#     formula = wasting ~ mo_pos_t_anomaly,
#     method = 'em',
#     accelerate = 'squarem' # these two options speed the convergence
#   )
# 
# logbin_crude_t_pos_mo %>% 
#   broom::tidy()

```

There are `r nrows(nsp_anomalies_wk_lag)` observations with a prior week from which a 1-lagged weekly tmax anomaly can be calculated. Let's write a function that creates a DF with lags 0 through 8 now.

## Adjusted log-binomial models with lags

```{r adj_lags}

## creating df
adj_lag_data =
  nsp_anomalies_wk_lag %>% 
  mutate(
    surv_area = as.factor(surv_area),
    woy = as.factor(woy),
    year = as.factor(year),
    moy = as.factor(moy)
  ) %>% 
  select(everything(), 
         lag0_t = wk_pos_t_anomaly,
         lag0_p_pos = wk_pos_p_anomaly,
         lag0_p_neg = wk_neg_p_anomaly)


adj_lags = function(beta1) {
  set.seed(1) ## going to sample to speed this up, so need consistent seed set
  
  df =
    adj_lag_data %>% 
    sample_frac(.1) %>% ## taking .1 of the data
    select(
      y = wasting,
      x = beta1,
      surv_area, moy, year
    )
  
  lag_fit =
    logbin(
      data = df,
      formula = y ~ x +    ## substitute vector of lag varnames for x
        surv_area + moy + year, ## region, season, and year are main potential confounders
      method = 'em',
      accelerate = 'squarem' # these two options speed the convergence
    ) %>% 
  
  # lag_fit2 = 
  #   lag_fit %>% 
    broom::tidy() %>% 
    filter(
      term == 'x'
    ) %>% 
    mutate(
      upper = estimate + 1.96 * std.error,
      lower = estimate - 1.96 * std.error
    ) %>% 
    select(-term)
  
  lag_fit
  
}

## POS T

## to create this, I commented out the covariates in the logbin model above (in the function) - figure out a reproducible way
# test_results_crude_t =
#   tibble(beta1_df = c('lag0_t','lag1_t','lag2_t','lag3_t','lag4_t','lag5_t','lag6_t','lag7_t','lag8_t','lag9_t','lag10_t','lag11_t','lag12_t','lag13_t','lag14_t','lag15_t','lag16_t')) %>% 
#   mutate(
#     output_lists = map(.x = beta1_df, ~adj_lags(beta1 = .x)),
#     estimates = map(output_lists, bind_rows)) %>% 
#   select(-output_lists) %>% 
#   unnest(estimates) %>% 
#   janitor::clean_names()
# 
# test_results_adj_t =
#   tibble(beta1_df = c('lag0_t','lag1_t','lag2_t','lag3_t','lag4_t','lag5_t','lag6_t','lag7_t','lag8_t','lag9_t','lag10_t','lag11_t','lag12_t','lag13_t','lag14_t','lag15_t','lag16_t')) %>% 
#   mutate(
#     output_lists = map(.x = beta1_df, ~adj_lags(beta1 = .x)),
#     estimates = map(output_lists, bind_rows)) %>% 
#   select(-output_lists) %>% 
#   unnest(estimates) %>% 
#   janitor::clean_names()

## POS PRECIP

# test_results_crude_p_pos =
#   tibble(beta1_df = c('lag0_p_pos','lag1_p_pos','lag2_p_pos','lag3_p_pos','lag4_p_pos','lag5_p_pos','lag6_p_pos','lag7_p_pos','lag8_p_pos','lag9_p_pos','lag10_p_pos','lag11_p_pos','lag12_p_pos','lag13_p_pos','lag14_p_pos','lag15_p_pos','lag16_p_pos')) %>%
#   mutate(
#     output_lists = map(.x = beta1_df, ~adj_lags(beta1 = .x)),
#     estimates = map(output_lists, bind_rows)) %>%
#   select(-output_lists) %>%
#   unnest(estimates) %>%
#   janitor::clean_names()
# 
# test_results_adj_p_pos =
#   tibble(beta1_df = c('lag0_p_pos','lag1_p_pos','lag2_p_pos','lag3_p_pos','lag4_p_pos','lag5_p_pos','lag6_p_pos','lag7_p_pos','lag8_p_pos','lag9_p_pos','lag10_p_pos','lag11_p_pos','lag12_p_pos','lag13_p_pos','lag14_p_pos','lag15_p_pos','lag16_p_pos')) %>%
#   mutate(
#     output_lists = map(.x = beta1_df, ~adj_lags(beta1 = .x)),
#     estimates = map(output_lists, bind_rows)) %>%
#   select(-output_lists) %>%
#   unnest(estimates) %>%
#   janitor::clean_names()


## NEG PRECIP

# test_results_crude_p_neg =
#   tibble(beta1_df = c('lag0_p_neg','lag1_p_neg','lag2_p_neg','lag3_p_neg','lag4_p_neg','lag5_p_neg','lag6_p_neg','lag7_p_neg','lag8_p_neg','lag9_p_neg','lag10_p_neg','lag11_p_neg','lag12_p_neg','lag13_p_neg','lag14_p_neg','lag15_p_neg','lag16_p_neg')) %>% 
#   mutate(
#     output_lists = map(.x = beta1_df, ~adj_lags(beta1 = .x)),
#     estimates = map(output_lists, bind_rows)) %>% 
#   select(-output_lists) %>% 
#   unnest(estimates) %>% 
#   janitor::clean_names()
# 
test_results_adj_p_neg =
  tibble(beta1_df = c('lag0_p_neg','lag1_p_neg','lag2_p_neg','lag3_p_neg','lag4_p_neg','lag5_p_neg','lag6_p_neg','lag7_p_neg','lag8_p_neg','lag9_p_neg','lag10_p_neg','lag11_p_neg','lag12_p_neg','lag13_p_neg','lag14_p_neg','lag15_p_neg','lag16_p_neg')) %>%
  mutate(
    output_lists = map(.x = beta1_df, ~adj_lags(beta1 = .x)),
    estimates = map(output_lists, bind_rows)) %>%
  select(-output_lists) %>%
  unnest(estimates) %>%
  janitor::clean_names()

## saveRDS(test_results_crude, file = "./logbin_results/test_crude.RDS")
test_results_crude2_t =
  readRDS(file = "./logbin_results/test_crude.RDS")

## saveRDS(test_results_adj, file = "./logbin_results/test_adj.RDS")
test_results_adj2_t =
  readRDS(file = "./logbin_results/test_adj.RDS")

## saveRDS(test_results_adj_p_pos, file = "./logbin_results/test_adj_p_pos.RDS")
test_results_adj_p_pos =
  readRDS(file = "./logbin_results/test_adj_p_pos.RDS")

## saveRDS(test_results_adj_p_neg, file = "./logbin_results/test_adj_p_neg.RDS")
test_results_adj_p_neg =
  readRDS(file = "./logbin_results/test_adj_p_neg.RDS")

```

## Visualizing crude + adjusted results

```{r viz}

## crude plot
test_results_crude_p_pos %>% 
  mutate(
    beta1_df = as.ordered(beta1_df),
    time = 0:16,
    beta1_df = fct_reorder(beta1_df, time)) %>% 
  ggplot(
    aes(x = beta1_df,
        y = exp(estimate),
        ymin = exp(lower),
        ymax = exp(upper))
  ) +
    geom_point() +
    geom_errorbar() +
    geom_hline(yintercept = 1) +
    theme_light() +
  labs(title = "crude")


## adj plot
test_results_adj_p_neg %>% 
  mutate(
    beta1_df = as.ordered(beta1_df),
    time = 0:16,
    beta1_df = fct_reorder(beta1_df, time)) %>% 
  ggplot(
    aes(x = beta1_df,
        y = exp(estimate),
        ymin = exp(lower),
        ymax = exp(upper))
  ) +
    geom_point() +
    geom_errorbar() +
    geom_hline(yintercept = 1) +
    theme_light() +
  labs(title = "adjusted")

```

